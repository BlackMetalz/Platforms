# This is basic for read data from kafka topic and send to elasticsearch
```
# read data from kafka topic
# More information go here: https://github.com/fluent/fluent-plugin-kafka
<source>
  @type kafka

  brokers 123.26.33.29:9092,123.26.33.31:9092,123.26.33.32:9092,123.26.33.33:9092
  topics kafka-topic
  format json
#  message_key <key (Optional, for text format only, default is message)>
#  add_prefix <tag prefix (Optional)>
#  add_suffix <tag suffix (Optional)>

  # Optionally, you can manage topic offset by using zookeeper
#  offset_zookeeper    <zookeer node list (<zookeeper1_host>:<zookeeper1_port>,<zookeeper2_host>:<zookeeper2_port>,..)>
#  offset_zk_root_node <offset path in zookeeper> default => '/fluent-plugin-kafka'

  # ruby-kafka consumer options
#  max_bytes     (integer) :default => nil (Use default of ruby-kafka)
#  max_wait_time (integer) :default => nil (Use default of ruby-kafka)
#  min_bytes     (integer) :default => nil (Use default of ruby-kafka)
</source>

# send logs to elastic search
<match>
  @type copy
  <store>
    @type elasticsearch
    #include_tag_key  true
    hosts 10.3.104.30:9200,10.3.105.128:9200
    user admin
    password yourrandompassword
    logstash_prefix "kienlt-test-fluentd"
    logstash_format true
    logstash_dateformat  %Y-%m-%d

    <buffer>
      @type file
      path /fluentd/log/buffer/cluster.buffer
      flush_mode interval
      flush_interval 10s
      flush_thread_count 16
      queued_chunks_limit_size 300
    </buffer> 
    slow_flush_log_threshold 40.0
  </store>
</match>
```
